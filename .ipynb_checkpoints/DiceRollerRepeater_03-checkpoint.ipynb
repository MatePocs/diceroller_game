{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3rd iteration of the project, building a temporal difference learning training set\n",
    "\n",
    "also renaming player to agent because apparently that is what you do\n",
    "\n",
    "heavily relying on this repository: \n",
    "https://github.com/ltbringer/tic_tac_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Creates an agent, sets config parameters, runs trainings.\n",
    "    \"\"\"\n",
    "    \n",
    "    agent = Agent(strategy = \"TD\")\n",
    "\n",
    "    # configuration parameters\n",
    "    \n",
    "    # how many times training is ran\n",
    "    number_of_trainings = 1000000\n",
    "    \n",
    "    # the type of dicerolling game we are playing\n",
    "    number_of_diceroll = 5\n",
    "    \n",
    "    train(agent, number_of_trainings, number_of_diceroll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, number_of_trainings, number_of_dicerolls):\n",
    "    \n",
    "    agent_results = {'mdl': agent, 'wins':0}\n",
    "    \n",
    "    for i in range(number_of_trainings):\n",
    "        \n",
    "        game = Game(number_of_dicerolls)\n",
    "        \n",
    "        while not game.over:\n",
    "            \n",
    "            # first, player looks at the state of the game and makes a decision\n",
    "            player_move = agent.select_move(game)\n",
    "            \n",
    "            # then, we insert that decision into the game\n",
    "            # in this case, it is a true or false decision\n",
    "            game.play_one_round(player_move)\n",
    "        \n",
    "\n",
    "        # once the game is over, optimize agent\n",
    "        reward = game.die_roll_to_reward(game.current_die_roll)\n",
    "        agent.on_reward(reward)\n",
    "\n",
    "    for key in sorted(agent.states.keys()):\n",
    "        print(key, agent.states[key], agent.states[key][0,0] < agent.states[key][0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Game:\n",
    "    \n",
    "    def __init__(self, number_of_dicerolls):\n",
    "        \n",
    "        self.number_of_dicerolls = number_of_dicerolls\n",
    "        \n",
    "        self.over = False\n",
    "        self.current_player_decision = True\n",
    "        self.current_round = 1\n",
    "        \n",
    "        #initiate game with one opening roll\n",
    "        self.current_die_roll = self.die_roll()\n",
    "        \n",
    "    def die_roll(self, side = 6):\n",
    "        \"\"\"\n",
    "        Returns a random integer i, 1 <= i <= side\n",
    "        uniform distribution\n",
    "        side is 6 by default, so it simulates 1d6\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.random.randint(1,side+1)\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        \"\"\"\n",
    "        Checks if the game is over, which can happen two ways:\n",
    "        - either we rolled the last dice, and there is no more player decision\n",
    "        - or the latest player decision was False\n",
    "        \"\"\"\n",
    "        \n",
    "        if (self.remaining_rounds() <= 0) or (not(self.current_player_decision)):\n",
    "            gameover =  True\n",
    "        else:\n",
    "            gameover = False\n",
    "        return gameover\n",
    "    \n",
    "    def advance_one_round(self):\n",
    "        self.current_round += 1\n",
    "        \n",
    "    def remaining_rounds(self):\n",
    "        return self.number_of_dicerolls - self.current_round\n",
    "    \n",
    "    def play_one_round(self, player_move):\n",
    "        \"\"\"\n",
    "        Checks player move is True or False\n",
    "        if True, rolls another die\n",
    "        if False, stops the game\n",
    "        \"\"\"\n",
    "        \n",
    "        # player move re-coded as [0,1] = True, [1,0] = False\n",
    "        \n",
    "        if np.array_equal(player_move, np.array([1,0])):\n",
    "            self.current_player_decision = False\n",
    "        else: \n",
    "            self.current_player_decision = True\n",
    "        \n",
    "        if self.current_player_decision == True:\n",
    "            self.advance_one_round()\n",
    "            self.current_die_roll = self.die_roll()\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        # also checks if game is over at this point\n",
    "        self.over = self.is_game_over()\n",
    "        \n",
    "    def die_roll_to_reward(self, die_roll_value):\n",
    "        \"\"\"\n",
    "        Recodes results to rewards spanning from -1 to +1\n",
    "        \"\"\"\n",
    "        if die_roll_value == 1:\n",
    "            reward = -1\n",
    "        elif die_roll_value == 2:\n",
    "            reward = - 0.6\n",
    "        elif die_roll_value == 3:\n",
    "            reward = - 0.2\n",
    "        elif die_roll_value == 4:\n",
    "            reward = 0.2\n",
    "        elif die_roll_value == 5:\n",
    "            reward = 0.6\n",
    "        elif die_roll_value == 6:\n",
    "            reward = 1\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent: \n",
    "    \"\"\"\n",
    "    Agent playing the game.\n",
    "    Keeps track of the prior experiments, makes a decision which can be exploration or exploitation. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, exploration_rate=1, decay=0.01, learning_rate=0.005, discount_factor=1, min_exploration_rate = 0.1,\n",
    "        strategy = \"random\"\n",
    "    ):\n",
    "        \n",
    "        self.states = {}\n",
    "        # dictionary of states, each state key is built from two integer: \n",
    "        # (current_die_value, remaining_dice_rolls)\n",
    "        # handled by staticmethod serialize_game\n",
    "        \n",
    "        self.state_order = []\n",
    "        # order of the states agent went through, empty the first time an agent is created\n",
    "        # will be emptied after each game too, when we handle the rewards\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        \n",
    "        self.strategy = strategy\n",
    "    \n",
    "    def select_move(self, game):\n",
    "        \"\"\"\n",
    "        Takes in state of the game, which consists of two integers:\n",
    "            - current_die_roll: the value showing on current die\n",
    "            - remaining_rolls: number of potential dice rolls after the current one\n",
    "        If player decides FALSE, the game stops, and the current_die_roll is the outcome\n",
    "        If player decides TRUE, the game continues with new die roll and -1 remaining_rolls\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "        # obviously build this up lol, for now, testing environment\n",
    "        \n",
    "        if self.strategy == \"external_strategy\":\n",
    "            if game.current_die_roll > 4:\n",
    "                decision = False\n",
    "            else:\n",
    "                decision = True\n",
    "        elif self.strategy == \"random\":\n",
    "            decision=np.random.choice([True, False])\n",
    "        elif self.strategy == \"devmode\":\n",
    "            #testing out different functions\n",
    "            #right now: will it pick up the 4-2 optimal move is True, if it plays correctly in last round\n",
    "            if game.remaining_rounds() == 1:\n",
    "                if game.current_die_roll > 4: \n",
    "                    decision = False\n",
    "                else:\n",
    "                    decision = True\n",
    "            else:\n",
    "                decision=np.random.choice([True, False])\n",
    "        elif self.strategy == \"TD\":\n",
    "            p =  np.random.uniform(0,1)\n",
    "            exploration = (p < self.exploration_rate)\n",
    "            if exploration == True:\n",
    "                decision = self.explore()\n",
    "            else:\n",
    "                decision = self.exploit(game)\n",
    "        \n",
    "        # action is True or False, coded as: \n",
    "        # True: [0, 1]\n",
    "        # False: [1, 0]\n",
    "        \n",
    "        if decision == False:\n",
    "            action = np.array([1,0])\n",
    "        else: \n",
    "            action = np.array([0,1])\n",
    "        \n",
    "        # stores the state - action pair the player made for this step\n",
    "        self.set_state(game, action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def set_state(self, game, action):\n",
    "        \"\"\"\n",
    "        store the action performed in a given state of the game\n",
    "        \"\"\"\n",
    "        state_key = Agent.serialize_game(game)\n",
    "        self.state_order.append((state_key, action))\n",
    "\n",
    "    \n",
    "    def on_reward(self, reward_value):\n",
    "        \"\"\"\n",
    "        Reward is the die roll that we had at the end of the game. \n",
    "        The higher the better. \n",
    "        \"\"\"        \n",
    "        while self.state_order:\n",
    "            # while there are states saved in the order list, keeps going \n",
    "            \n",
    "            state_key, action = self.state_order.pop()\n",
    "            # get the state and action pairs\n",
    "            \n",
    "            old_reward = self.states.get(state_key, np.zeros((1,2)))\n",
    "            # old state is either whatever was in that state previously, or an empty 1*2 matrix\n",
    "            \n",
    "            reward_increment = self.learn_by_temporal_difference(old_reward, reward_value, state_key, action)\n",
    "            # reward increment and old_reward are a 1 by 2 np array, \n",
    "            # first element: the cumulative reward for cases when the chosen action was False\n",
    "            # second: cumulative reward for cases when action was True\n",
    "            \n",
    "            self.states[state_key] = old_reward + reward_increment\n",
    "            \n",
    "            reward_value *= self.discount_factor\n",
    "            # reduce the reward after each step\n",
    "            \n",
    "#             self.log_analysis(state_key, action, reward_value, old_reward, reward_increment)\n",
    "            \n",
    "        self.exploration_rate = max(self.exploration_rate - self.decay, self.min_exploration_rate)\n",
    "        # we decrease future exploration rate in this step\n",
    "\n",
    "    def learn_by_temporal_difference(self, old_reward, reward_value, state_key, action):\n",
    "\n",
    "        \n",
    "        return self.learning_rate * (((reward_value * action) - old_reward) * action)\n",
    "        # following temporal learning formula\n",
    "        # reward value * action: makes a 1 x 2 matrix from the reward value\n",
    "        # minus old_reward: subtract the old saved rewards\n",
    "        # * action again: we are only interested in the parts of the matrix that was impacted by the action\n",
    "        # everything else is zeroed out, because we want to keep the old reward values there, considering\n",
    "        # we dont have new information regarding that state - action pair\n",
    "        \n",
    "    def explore(self):\n",
    "        \"\"\"\n",
    "        Randomly explores the possible actions, without considering the history.\n",
    "        In our case, this is a random True or False.\n",
    "        \"\"\"\n",
    "        decision = np.random.choice([True, False])\n",
    "        return decision\n",
    "    \n",
    "    def exploit(self, game):\n",
    "        \"\"\"\n",
    "        Checks if we have any prior information regarding this particular state. \n",
    "        If not, reverts back to exploration. \n",
    "        If yes, picks the action with the highest cumulated reward assigned to it. \n",
    "        \"\"\"\n",
    "        state_key = Agent.serialize_game(game)\n",
    "        if state_key in self.states:\n",
    "            historical_reward = self.states[state_key]\n",
    "            if historical_reward[0,0] > historical_reward[0,1]:\n",
    "                decision = False\n",
    "            else:\n",
    "                decision = True\n",
    "            #TODO find a way to make this more robust    \n",
    "            \n",
    "#             print(historical_reward)\n",
    "#             print(decision)\n",
    "            \n",
    "        else:\n",
    "            decision = self.explore()\n",
    "            #even if we wanted to exploit, there is nothing to move on, revert to explore\n",
    "            \n",
    "        return decision\n",
    "    \n",
    "    @staticmethod\n",
    "    def serialize_game(game):\n",
    "        \"\"\"\n",
    "        For now, it is very simple, game state is defined by two integers: \n",
    "        current_die_roll, remaining rounds\n",
    "        Returns these in a string with a \"-\" in the middle\n",
    "        current_die_roll = 3 and remaining_rounds = 12 returns \"3-12\"\n",
    "        \"\"\"\n",
    "        current_die_roll = game.current_die_roll\n",
    "        remaining_rounds = game.remaining_rounds()\n",
    "        return str(current_die_roll) + \"-\" + str(remaining_rounds)\n",
    "    \n",
    "    def log_analysis(self, state_key, action, reward_value, old_reward, reward_increment):\n",
    "        \"\"\"\n",
    "        Created to track how one key gets the final value. \n",
    "        \"\"\"\n",
    "        if True:\n",
    "            print(f\"{state_key} key was encountered in this round\")\n",
    "            print(f\"action chosen: {action}\")\n",
    "            print(f\"reward to give to all states was : {reward_value}\")\n",
    "            print(f\"old cumulative reward was: {old_reward}\")\n",
    "            print(f\"reward is going to be increased by: {reward_increment}\")\n",
    "            print(f\"so now the cumulative reward is: {self.states[state_key]}\")\n",
    "            print(\"\")\n",
    "            \n",
    "            if (state_key[-1] == str(2)) or (action[1] == 1 ):\n",
    "                print(\"--------------------------\")\n",
    "                print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1 [[-0.99997397 -0.01910152]] True\n",
      "1-2 [[-0.99999998  0.25753547]] True\n",
      "1-3 [[-1.          0.40410508]] True\n",
      "1-4 [[-1.          0.53263981]] True\n",
      "2-1 [[-0.59999153 -0.01650078]] True\n",
      "2-2 [[-0.59999999  0.2838495 ]] True\n",
      "2-3 [[-0.6         0.38229775]] True\n",
      "2-4 [[-0.6         0.49330816]] True\n",
      "3-1 [[-0.19999737 -0.00587907]] True\n",
      "3-2 [[-0.19999999  0.22689457]] True\n",
      "3-3 [[-0.2         0.37763388]] True\n",
      "3-4 [[-0.2        0.4962943]] True\n",
      "4-1 [[0.2        0.03144677]] False\n",
      "4-2 [[0.2        0.23823287]] True\n",
      "4-3 [[0.2        0.47342095]] True\n",
      "4-4 [[0.2       0.5217909]] True\n",
      "5-1 [[0.6        0.02504011]] False\n",
      "5-2 [[0.6        0.23787624]] False\n",
      "5-3 [[0.6        0.42414841]] False\n",
      "5-4 [[0.6        0.49288091]] False\n",
      "6-1 [[ 1.         -0.02222511]] False\n",
      "6-2 [[1.         0.28468875]] False\n",
      "6-3 [[1.         0.37943137]] False\n",
      "6-4 [[1.        0.5282986]] False\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1000000000000005"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min exploration rate of 0.2 means that with 0.2 probability, we will stay at 3.5 expected value\n",
    "0.2 * 3.5 + 0.8 * 4.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#still more than 4, should converge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
